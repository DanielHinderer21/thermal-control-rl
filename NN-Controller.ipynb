{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb6ed257-6c06-4fb1-8405-e63adeee3840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   31.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 64 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "###########  Starting RL  ###########\n",
      "\n",
      " ########### I: 0 ############\n",
      "  \n",
      " Erstelle 3x2-Subplot-Raster \n",
      "\n",
      " ########### I: 1 ############\n",
      "\n",
      " ########### I: 2 ############\n",
      "  \n",
      " Erstelle 3x2-Subplot-Raster \n",
      "\n",
      " ########### I: 3 ############\n",
      "\n",
      " ########### I: 4 ############\n",
      "  \n",
      " Erstelle 3x2-Subplot-Raster \n"
     ]
    }
   ],
   "source": [
    "######################  Config start\n",
    "\n",
    "#Ausgaben speichern unter Name\n",
    "notebook_name = 'NN-Controller_Example'\n",
    "\n",
    "#Netzwerkarchitektur\n",
    "num_hidden_neurons = 5\n",
    "input_num = 13\n",
    "\n",
    "#Metaparameter Konfiguration\n",
    "mue_alpha = 0.08\n",
    "sigma_alpha = 0.04\n",
    "num_iterations = 8000\n",
    "make_plot_each = 50\n",
    "take_test_data_every = 100\n",
    "update_feat_every = 500\n",
    "no_feat_imp_bevore = 2300\n",
    "\n",
    "#Eingabeparameter Aktivieren oder Deaktivieren\n",
    "delta_Ti_2h_inp = 1\n",
    "post_Ta_avg_32h_inp = 1\n",
    "Tw_inp = 1\n",
    "Te_inp = 1\n",
    "price_flag = 1\n",
    "no_future_inp = 1\n",
    "dif_price_inp = 0\n",
    "\n",
    "#Übersetzung der Ausgabe in Vorlauftemperatur\n",
    "num_outputs = 2\n",
    "dif = 0\n",
    "change_rate = 0.02\n",
    "\n",
    "#Heizungsregelungsbereich\n",
    "max_t_eval = 23.5\n",
    "min_t_eval = 22.0\n",
    "\n",
    "#Kosten Konfiguration\n",
    "price_factor = 0.5\n",
    "\n",
    "\n",
    "#########################  Config end\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/XXXX/Daniel\")\n",
    "\n",
    "# Pfad zum Ordner RL_Results\n",
    "rl_results_path = os.path.join(os.getcwd(), \"XXXX\")\n",
    "\n",
    "# Überprüfen, ob der Ordner RL_Results existiert, wenn nicht, erstellen\n",
    "if not os.path.exists(rl_results_path):\n",
    "    os.makedirs(rl_results_path)\n",
    "\n",
    "# Pfad für den neuen Ordner innerhalb von RL_Results\n",
    "new_folder_path = os.path.join(rl_results_path, notebook_name )\n",
    "\n",
    "# Erstellen des neuen Ordners, wenn er nicht existiert\n",
    "if not os.path.exists(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "new_folder_path = new_folder_path + \"/\"\n",
    "\n",
    "#sys.path.append(r'C:\\Users\\dhindere\\Master-Thesis\\Temp_Model\\RC-Model_Temp\\ZSW_DATA\\optimization_lite-main')\n",
    "\n",
    "from datetime import timedelta \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "import RC_Modell.ownUtils\n",
    "from RC_Modell.models import DarkGreyModel, Ti, TiTe, TiTeTwSInoSIso\n",
    "from RC_Modell.plot import plot, plot_input_data, plot_with_Th\n",
    "from RC_Modell.fit import darkgreyfit\n",
    "from RC_Modell.error import rmse\n",
    "from RC_Modell.predict import predict_model\n",
    "\n",
    "from black_box_optimizers.pgpe import RankSuperSymPGPE\n",
    "from optimization.optimizer import OptimizeableParameterDict as OP\n",
    "\n",
    "\n",
    "\n",
    "#Funktionen\n",
    "\n",
    "def make_weights_biases_from_vector(para_vector, structure):\n",
    "    weights_biases_list = []\n",
    "    count = 0\n",
    "    for i, v in enumerate(structure):\n",
    "        \n",
    "        # weights\n",
    "        cur_weights = np.zeros((v, structure[i+1]), 'float32')\n",
    "        s = cur_weights.size\n",
    "        cur_weights = para_vector[count:count+s].reshape(cur_weights.shape)\n",
    "        count += s\n",
    "        weights_biases_list.append(cur_weights)\n",
    "        \n",
    "        # biases\n",
    "        cur_weights = np.zeros((structure[i+1],), 'float32')\n",
    "        s = cur_weights.size\n",
    "        cur_weights = para_vector[count:count+s].reshape(cur_weights.shape)\n",
    "        count += s\n",
    "        weights_biases_list.append(cur_weights)\n",
    "        \n",
    "        if i == len(structure)-2:\n",
    "            break\n",
    "        \n",
    "    net_params = {\"num_layers\": len(structure)-2, \"activation\": \"sigmoid\", \"out_type\": \"linear\"}\n",
    "    return weights_biases_list, net_params\n",
    "\n",
    "def get_num_paras_from_structure(structure):\n",
    "    count = 0\n",
    "    for i, v in enumerate(structure):\n",
    "        # weights\n",
    "        cur_weights = np.zeros((v, structure[i+1]), 'float32')\n",
    "        s = cur_weights.size\n",
    "        count += s\n",
    "        \n",
    "        # biases\n",
    "        cur_weights = np.zeros((1, structure[i+1]), 'float32')\n",
    "        s = cur_weights.size\n",
    "        count += s\n",
    "        \n",
    "        if i == len(structure)-2:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def forward_path_nnr(inputs, weights_biases, net_params):\n",
    "    \n",
    "    function_mapping = {\n",
    "        \"sigmoid\": lambda x: 1 / (1 + np.exp(-x)),\n",
    "        \"leakyReLU\": lambda x: ((x > 0) * x) + ((x <= 0) * x * 0.01),\n",
    "        \"tanh\": lambda x: np.sinh(x) / np.cosh(x),\n",
    "        \"linear\": lambda x: x,\n",
    "    }\n",
    "\n",
    "    curr_layer = inputs.copy()\n",
    "    weights = weights_biases[::2]\n",
    "    biases = weights_biases[1::2]\n",
    "    for l in range(net_params[\"num_layers\"]):\n",
    "        curr_layer = np.dot(curr_layer, weights[l]) + biases[l].reshape(-1,biases[l].shape[-1])\n",
    "        curr_layer = function_mapping[net_params[\"activation\"]](curr_layer)\n",
    "    output = np.dot(curr_layer, weights[-1]) + biases[-1]\n",
    "    output = function_mapping[net_params[\"out_type\"]](output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def custom_splits(X, num_sets, train_len, val_len, variance):\n",
    "    \n",
    "    total_len = train_len + val_len\n",
    "    max_index = len(X)\n",
    "    \n",
    "    # Calculate step size to distribute the sets across the data\n",
    "    step = (max_index - total_len) // num_sets\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    for i in range(num_sets):\n",
    "        # Start index for this set\n",
    "        start_index = i * step\n",
    "        \n",
    "        # Randomly vary the training length\n",
    "        random_variance = np.random.randint(-variance, variance + 1)\n",
    "        current_train_len = train_len + random_variance\n",
    "        \n",
    "        # Ensure we don't exceed the max index\n",
    "        if start_index + current_train_len + val_len > max_index:\n",
    "            break\n",
    "        \n",
    "        train_indices = list(range(start_index, start_index + current_train_len))\n",
    "        val_indices = list(range(start_index + current_train_len, start_index + current_train_len + val_len))\n",
    "        \n",
    "        splits.append((np.array(train_indices), np.array(val_indices)))\n",
    "        \n",
    "    return splits\n",
    "\n",
    "\n",
    "def select_model_by_position(df, position):\n",
    "    model_name = df.iloc[position][('train', 'model')]\n",
    "    return model_name\n",
    "\n",
    "def extract_random_sequence(input_df, days):\n",
    "    # Anzahl der Zeilen, die einem Tag entsprechen (angenommen, die Daten sind stündlich)\n",
    "    rows_per_day = 24\n",
    "    \n",
    "    # Maximaler Startindex, damit die Sequenz von X Tagen nicht den Datenrahmen übersteigt\n",
    "    max_start_idx = len(input_df) - days * rows_per_day\n",
    "    \n",
    "    # Zufälligen Startindex wählen\n",
    "    start_idx = random.randint(0, max_start_idx)\n",
    "    \n",
    "    # Ende-Index berechnen\n",
    "    end_idx = start_idx + days * rows_per_day\n",
    "    \n",
    "    # Sequenz extrahieren\n",
    "    sequence_df = input_df.iloc[start_idx:end_idx, :]\n",
    "    \n",
    "    plot_input_data(sequence_df, with_Kh = False, with_Ph = False)\n",
    "    \n",
    "    # Extract input features and target variable\n",
    "    X_test_test = input_df[['Th', 'Ta', 'SIno', 'SIso', 'Ti0', 'Te0', 'Tw0']]\n",
    "    y_test_test = input_df['Ti']\n",
    "    \n",
    "    return X_test_test, y_test_test\n",
    "\n",
    "def rc_model_predict(sequence_df, model, error_metric, Te0):\n",
    "    # Extrahiere die notwendigen Anfangsbedingungen und Parameter\n",
    "    X = sequence_df.drop('Ti', axis=1)\n",
    "    y = sequence_df['Ti']\n",
    "    \n",
    "    ic_params_map = {\n",
    "       'Ti0': lambda X, y, model_result: y.iloc[0],\n",
    "       'Th0': lambda X, y, model_result: y.iloc[0],\n",
    "       'Te0': lambda X, y, model_result: Te0,    # Verwende den berechneten Wert von Te0\n",
    "    }\n",
    "    \n",
    "    # Führe die Vorhersage durch\n",
    "    prediction_df = predict_model(model, X, y, ic_params_map, error_metric, model.result)\n",
    "    predictions = prediction_df['model_result'].iloc[0].var['Ti']\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def find_min_max_values(df):  \n",
    "    min_max_values = {}\n",
    "    all_top_limits = []\n",
    "    all_bot_limits = []\n",
    "    überschuss = 0.1\n",
    "\n",
    "    # Berechnung nur für 'Ti', 'Ta', 'Th'\n",
    "    for col in ['Ti', 'Ta']:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        dif = max_val - min_val\n",
    "        top_limit = max_val + dif * überschuss\n",
    "        bot_limit = min_val - dif * überschuss\n",
    "\n",
    "        min_max_values[col] = {'min': bot_limit, 'max': top_limit}\n",
    "        all_top_limits.append(top_limit)\n",
    "        all_bot_limits.append(bot_limit)\n",
    "\n",
    "    # Finde den höchsten top_limit und den niedrigsten bot_limit unter 'Ti', 'Ta', 'Th'\n",
    "    max_top_limit = max(all_top_limits)\n",
    "    min_bot_limit = min(all_bot_limits)\n",
    "\n",
    "    # Setze diese Werte für 'Te' und 'Tw'\n",
    "    min_max_values['Te'] = {'min': min_bot_limit, 'max': max_top_limit}\n",
    "    min_max_values['Tw'] = {'min': min_bot_limit, 'max': max_top_limit}\n",
    "    min_max_values['Th'] = {'min': -3, 'max': 3}\n",
    "\n",
    "    # Füge auch 'SIno' und 'SIso' hinzu, aber ohne sie für 'Te' und 'Tw' zu berücksichtigen\n",
    "    for col in ['SIno', 'SIso']:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        dif = max_val - min_val\n",
    "        min_max_values[col] = {'min': min_val - dif * überschuss, 'max': max_val + dif * überschuss}\n",
    "\n",
    "    # Assuming a reasonable range for deltaTi_-2h and day_hours \n",
    "    min_max_values['deltaTi'] = {'min': -2, 'max': 2}\n",
    "    min_max_values['day_hours'] = {'min': 0, 'max': 24}\n",
    "\n",
    "    # Using the same range for averages as their original values\n",
    "    min_max_values['Ta_avg'] = min_max_values['Ta']\n",
    "    min_max_values['SIso_avg'] = min_max_values['SIso']     \n",
    "    min_max_values['SIno_avg'] = min_max_values['SIno']\n",
    "\n",
    "    # For price\n",
    "    max_price = df['price'].max()\n",
    "    min_price = df['price'].min()\n",
    "    if abs(max_price) > abs(min_price):\n",
    "        price_limit = max_price\n",
    "    else:\n",
    "        price_limit = min_price\n",
    "    min_max_values['price'] = {'min': -price_limit, 'max': price_limit}\n",
    "    \n",
    "    return min_max_values\n",
    "\n",
    "min_max_values = find_min_max_values(input_df)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "def denormalize_data(norm_value, min_value, max_value):\n",
    "    return norm_value * (max_value - min_value) + min_value\n",
    "\n",
    "def evaluate_temp(Ti,time_hour):\n",
    "    if start_working_hour < 6 or end_working_hour > 20: \n",
    "        return 0\n",
    "    abweichung = 0\n",
    "    if Ti <= max_t_eval and Ti >= min_t_eval:\n",
    "        return 0\n",
    "    elif Ti > max_t_eval:\n",
    "        abweichung = Ti - max_t_eval\n",
    "    elif Ti < min_t_eval:\n",
    "        abweichung = min_t_eval - Ti\n",
    "    return float(-(abweichung ** 2)*1.5)  # Negative quadratische Abweichung als Belohnung\n",
    "\n",
    "\n",
    "def evaluate_energy(bkt_an,bkt_vorlauftemp_set,Tw,bkt_state_last,price_now):\n",
    "    if bkt_an == 0:\n",
    "        return 0\n",
    "    initial_cost = 0\n",
    "    if bkt_state_last == 0:\n",
    "        initial_cost = 0.01\n",
    "    abweichung = abs(Tw - bkt_vorlauftemp_set)\n",
    "    energy_cost = float(-abweichung*0.01-initial_cost)\n",
    "    if price_flag == 1:\n",
    "        return energy_cost * price_now * price_factor\n",
    "    else:\n",
    "        return energy_cost * price_now * price_factor\n",
    "\n",
    "    \n",
    "def evaluate_vorlauf_range(bkt_vorlauftemp):\n",
    "    #abweichung = 0\n",
    "    #if bkt_vorlauftemp > (heiz_temp_max+5):\n",
    "    #    abweichung = bkt_vorlauftemp - (heiz_temp_max+5)\n",
    "    #elif bkt_vorlauftemp < (kühl_temp_min-5):\n",
    "    #    abweichung = (kühl_temp_min-5) - bkt_vorlauftemp\n",
    "    #return float(-(abweichung ** 2) * 0.01)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calculate_bkt_vorlauftemo(normed_nn_out,x_seq_copy,t,next_result):\n",
    "    \n",
    "    bkt_vorlauftemp_15 = 0\n",
    "    bkt_vorlauftemp_30 = 0\n",
    "\n",
    "    # Reale Heiz/Kühlgrenzen einsetzen\n",
    "    new_temp = 0\n",
    "\n",
    "    # Regelung mit Differenzen vom jetztwert:\n",
    "    if dif == 1:\n",
    "        if num_outputs == 1:\n",
    "            if bkt_vorlauftemp <= 5 and bkt_vorlauftemp >= -5:\n",
    "                new_temp = x_seq_copy['Th'].iloc[t]\n",
    "            bkt_an = 1\n",
    "        else:\n",
    "            if normed_nn_out[0][1] < 0:\n",
    "                bkt_an = 0\n",
    "            else:\n",
    "                bkt_an = 1\n",
    "                    \n",
    "        if bkt_an == 1:\n",
    "            if bkt_vorlauftemp > 5:\n",
    "                new_temp = x_seq_copy['Th'].iloc[t] + (bkt_vorlauftemp-5) * change_rate\n",
    "            elif bkt_vorlauftemp < -5:\n",
    "                new_temp = x_seq_copy['Th'].iloc[t] + (bkt_vorlauftemp+5) * change_rate\n",
    "    \n",
    "    else: # Regelung auf Festwert:\n",
    "        if num_outputs == 1:\n",
    "            bkt_an = 1\n",
    "        else:\n",
    "            if normed_nn_out[0][1] < 0:\n",
    "                bkt_an = 0\n",
    "            else:\n",
    "                bkt_an = 1\n",
    "                    \n",
    "        if bkt_an == 1:\n",
    "            if bkt_vorlauftemp <= 5 and bkt_vorlauftemp >= -5:\n",
    "                bkt_an = 0\n",
    "            elif bkt_vorlauftemp < -5:\n",
    "                new_temp = kühl_temp_max + (bkt_vorlauftemp+5) * change_rate\n",
    "            elif bkt_vorlauftemp > 5:\n",
    "                new_temp = heiz_temp_min + (bkt_vorlauftemp-5) * change_rate\n",
    "                \n",
    "    # Grenzen gelten für alle:    \n",
    "    if new_temp > heiz_temp_max:\n",
    "        new_temp = heiz_temp_max\n",
    "    elif new_temp < kühl_temp_min:\n",
    "        new_temp = kühl_temp_min\n",
    "    elif new_temp > kühl_temp_max and new_temp < heiz_temp_min:\n",
    "        bkt_an = 0\n",
    "                    \n",
    "    bkt_vorlauftemp_15 = new_temp\n",
    "    bkt_vorlauftemp_30 = new_temp\n",
    "\n",
    "    # Falls aus: \n",
    "    if bkt_an == 0:\n",
    "        min_in_sec_15min = 15*60\n",
    "        min_in_sec_30min = 30*60\n",
    "        bkt_vorlauftemp_15 = use_pt1_function(min_in_sec_15min, beharrungswert=next_result.var['Tw'][-1], start_temp=x_seq_copy['Th'].iloc[t])\n",
    "        bkt_vorlauftemp_30 = use_pt1_function(min_in_sec_30min, beharrungswert=next_result.var['Tw'][-1], start_temp=x_seq_copy['Th'].iloc[t])\n",
    "\n",
    "    return bkt_vorlauftemp_15, bkt_vorlauftemp_30\n",
    "\n",
    "\n",
    "def save_feat_csv(feat_results):\n",
    "    \n",
    "    value_names = [\"Ti\", \"Tw\", \"Te\", \"delta Ti 2h\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                   \"pre SIso avg 4h\", \"post SIso avg 8h\", \"post Ta avg 32h\", \"pre Th avg 0.5h\",\n",
    "                   \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "\n",
    "    if delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 1 and Te_inp == 1:\n",
    "    \n",
    "        value_names = [\"Ti\", \"Tw\", \"Te\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\",  \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "        \n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 0 and Te_inp == 1:\n",
    "    \n",
    "        value_names = [\"Ti\", \"Te\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\",  \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "\n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 1 and Te_inp == 0:\n",
    "    \n",
    "        value_names = [\"Ti\", \"Tw\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\",  \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "        \n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 0 and Te_inp == 0:\n",
    "    \n",
    "        value_names = [\"Ti\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\",  \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "\n",
    "    elif price_flag == 1 and no_future_inp == 0 and dif_price_inp == 0:\n",
    "        \n",
    "        value_names = [\"Ti\", \"Tw\", \"Te\", \"price\", \"price min in x\", \"price max in x\", \"delta Ti 2h\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\", \"post Ta avg 32h\", \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "\n",
    "    elif price_flag == 1 and no_future_inp == 1:\n",
    "        \n",
    "        value_names = [\"Ti\", \"Tw\", \"Te\", \"price\", \"delta Ti 2h\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\", \"post Ta avg 32h\", \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "\n",
    "    elif price_flag == 1 and dif_price_inp == 1:\n",
    "\n",
    "        value_names = [\"Ti\", \"Tw\", \"Te\", \"price\", \"delta price\", \"delta Ti 2h\", \"pre Ta avg 12h\", \"post Ta avg 24h\",\n",
    "                       \"pre SIso avg 4h\", \"post SIso avg 8h\", \"post Ta avg 32h\", \"pre Th avg 0.5h\",\n",
    "                       \"hour with minutes\", \"delta Tset\", \"original\"]\n",
    "    \n",
    "   # Negate values in feat_results and calculate sums for average calculation\n",
    "    sum_sensitivity = [0] * len(value_names)\n",
    "    sum_nullify = [0] * len(value_names)\n",
    "    for index, (sens, null) in enumerate(feat_results):\n",
    "        negated_sens = [-x for x in sens]\n",
    "        negated_null = [-x for x in null]\n",
    "        feat_results[index] = (negated_sens, negated_null)\n",
    "        sum_sensitivity = [sum_val + neg_val for sum_val, neg_val in zip(sum_sensitivity, negated_sens)]\n",
    "        sum_nullify = [sum_val + neg_val for sum_val, neg_val in zip(sum_nullify, negated_null)]\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_sensitivity = [val / len(feat_results) for val in sum_sensitivity]\n",
    "    avg_nullify = [val / len(feat_results) for val in sum_nullify]\n",
    "\n",
    "    # Add the average DataFrame so it can be treated like the others\n",
    "    feat_results.append((avg_sensitivity, avg_nullify))\n",
    "\n",
    "    # Speichern des 'original' Wertes für jede Population\n",
    "    original_values = []\n",
    "\n",
    "    for i, (sens, null) in enumerate(feat_results):\n",
    "        # Speichern des 'original' Wertes\n",
    "        original_sens = sens[-1]  # 'original' ist der erste Wert in der Liste\n",
    "        original_null = null[-1]  # Gleiches gilt für Nullify\n",
    "        original_values.append((original_sens, original_null))\n",
    "\n",
    "        # Subtrahieren des 'original' Wertes von allen anderen Werten\n",
    "        adjusted_sens = [x - original_sens for x in sens]\n",
    "        adjusted_null = [x - original_null for x in null]\n",
    "        adjusted_sens[0] = 0  # Setzen von 'original' auf 0\n",
    "        adjusted_null[0] = 0  # Gleiches für Nullify\n",
    "        feat_results[i] = (adjusted_sens, adjusted_null)\n",
    "\n",
    "    # Initialize a list to store DataFrames for each population\n",
    "    all_dataframes = []\n",
    "\n",
    "    for i, (sens, null) in enumerate(feat_results, start=1):\n",
    "        # Pair each feature name with its sensitivity and nullify values\n",
    "        paired_sens = list(zip(value_names, sens))\n",
    "        paired_null = list(zip(value_names, null))\n",
    "\n",
    "        # Find and move 'original' to the first position\n",
    "        original_sens_index = next(i for i, x in enumerate(paired_sens) if x[0] == 'original')\n",
    "        original_null_index = next(i for i, x in enumerate(paired_null) if x[0] == 'original')\n",
    "        original_sens = paired_sens.pop(original_sens_index)\n",
    "        original_null = paired_null.pop(original_null_index)\n",
    "\n",
    "        # Sort remaining features based on their values\n",
    "        sorted_sens = sorted(paired_sens, key=lambda x: x[1], reverse=True)\n",
    "        sorted_null = sorted(paired_null, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Insert 'original' at the first position\n",
    "        sorted_sens.insert(0, original_sens)\n",
    "        sorted_null.insert(0, original_null)\n",
    "\n",
    "        # Create separate lists for names and values\n",
    "        names_sens, values_sens = zip(*sorted_sens)\n",
    "        names_null, values_null = zip(*sorted_null)\n",
    "\n",
    "        # Convert to DataFrame format\n",
    "        df = pd.DataFrame({\n",
    "            f'Pop Sens Name {i}': names_sens,\n",
    "            f'Pop Sens Val {i}': values_sens,\n",
    "            f'Pop Nullify Name {i}': names_null,\n",
    "            f'Pop Nullify Val {i}': values_null\n",
    "        })\n",
    "\n",
    "        # Add the DataFrame to the list\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "        # Add an empty DataFrame to create a separator column\n",
    "        all_dataframes.append(pd.DataFrame({\" \": ['']*len(df)}))\n",
    "\n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(all_dataframes, axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Hinzufügen der 'old Original' Werte\n",
    "    original_df = pd.DataFrame({\n",
    "        'Name': ['old Original'] * len(original_values),\n",
    "        'Sens Value': [val[0] for val in original_values],\n",
    "        'Nullify Value': [val[1] for val in original_values]\n",
    "    })\n",
    "    combined_df = pd.concat([combined_df, pd.DataFrame({}), original_df], axis=1)\n",
    "\n",
    "    \n",
    "    # Bestimmen der maximalen Länge\n",
    "    max_length = max(len(combined_df), len(test_scores), len(test_scores_index))\n",
    "\n",
    "    # Leere Zeilen erstellen und an combined_df anhängen, falls notwendig\n",
    "    if len(combined_df) < max_length:\n",
    "        additional_rows = max_length - len(combined_df)\n",
    "        # Erstellen eines DataFrames mit leeren Zeilen\n",
    "        empty_df = pd.DataFrame([pd.Series()] * additional_rows)\n",
    "\n",
    "        # Vor dem Zusammenführen der DataFrames die Indizes zurücksetzen\n",
    "        combined_df.reset_index(drop=True, inplace=True)\n",
    "        empty_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Jetzt die DataFrames zusammenführen\n",
    "        combined_df = pd.concat([combined_df, empty_df], ignore_index=True)\n",
    "\n",
    "            \n",
    "    # Erstellen eines DataFrames für zusätzliche Informationen\n",
    "    additional_info_df = pd.DataFrame({\n",
    "        ' ': [' ', f'best_test_score= {best_test_score}', f'best_test= {best_test}'] + [''] * (max_length - 3),\n",
    "        'test_scores': test_scores + [''] * (max_length - len(test_scores)),\n",
    "        'test_scores_index': test_scores_index + [''] * (max_length - len(test_scores_index))\n",
    "    })\n",
    "    \n",
    "    # Hinzufügen des zusätzlichen Info-DataFrames am Ende\n",
    "    combined_df = pd.concat([combined_df, additional_info_df], axis=1)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    combined_df.to_csv(save_path + \"feat_result.csv\", index=False)\n",
    "\n",
    "    ### Feat Plot\n",
    "    # Funktion zum Extrahieren von Daten für eine bestimmte Population\n",
    "    def extract_pop_data(df, pop_num, data_type):\n",
    "        return df[[f'Pop {data_type} Name {pop_num}', f'Pop {data_type} Val {pop_num}']]\n",
    "\n",
    "    # Figur mit Subplots erstellen, vergrößere die Größe der Figur\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(35, 17))  # Größe angepasst\n",
    "\n",
    "    # Iteration über jede Population, um einen Plot zu erstellen\n",
    "    for i in range(5):\n",
    "        # Sens-Daten extrahieren\n",
    "        sens_data = extract_pop_data(combined_df, i + 1, \"Sens\")\n",
    "        names_sens = sens_data[f'Pop Sens Name {i + 1}'].dropna()\n",
    "        values_sens = sens_data[f'Pop Sens Val {i + 1}'].dropna()\n",
    "\n",
    "        # Nullify-Daten extrahieren\n",
    "        null_data = extract_pop_data(combined_df, i + 1, \"Nullify\")\n",
    "        names_null = null_data[f'Pop Nullify Name {i + 1}'].dropna()\n",
    "        values_null = null_data[f'Pop Nullify Val {i + 1}'].dropna()\n",
    "\n",
    "        # Sens-Daten plotten\n",
    "        axes[0, i].bar(names_sens, values_sens, alpha=0.6)\n",
    "        axes[0, i].set_title(f'Population {i + 1} Sens')\n",
    "        axes[0, i].set_xticks(range(len(names_sens)))\n",
    "        axes[0, i].set_xticklabels(names_sens, rotation=45, ha='right')\n",
    "\n",
    "        # Nullify-Daten plotten\n",
    "        axes[1, i].bar(names_null, values_null, alpha=0.6)\n",
    "        axes[1, i].set_title(f'Population {i + 1} Nullify')\n",
    "        axes[1, i].set_xticks(range(len(names_null)))\n",
    "        axes[1, i].set_xticklabels(names_null, rotation=45, ha='right')\n",
    "\n",
    "    # Gesamttitel und Layout einstellen\n",
    "    fig.suptitle('Feature Plots für Populationen (Sens und Nullify)', fontsize=20)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Speichern des Plots als Bild\n",
    "    plt.savefig(save_path + 'feat_plot.png')\n",
    "\n",
    "    ### Test Plot\n",
    "    # Erstellen des Plots\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_scores_index, test_scores, label='Test Scores', color='blue')\n",
    "    plt.xlabel('Iterationen')\n",
    "    plt.ylabel('Test Scores')\n",
    "    plt.title('Test Scores over Iterations')\n",
    "    \n",
    "    # Markieren des Punktes mit dem maximalen Wert\n",
    "    max_score_index = test_scores.index(max(test_scores))\n",
    "    plt.scatter(test_scores_index[max_score_index], test_scores[max_score_index], color='red', label='Maximaler Wert')\n",
    "    \n",
    "    # Legende hinzufügen\n",
    "    plt.legend()\n",
    "    \n",
    "    # Speichern des Plots als Bild\n",
    "    plt.savefig(save_path + 'test_scores_plot.png')\n",
    "\n",
    "def create_new_input(x_seq_copy,t,next_result):\n",
    "    \n",
    "    # Calculate new features\n",
    "    deltaTi_2h = x_seq_copy['Ti'].iloc[t+2] - x_seq_copy['Ti'].iloc[t+2-2*4]  # Ti_now - Ti_old\n",
    "    Ta_avg_last12h = x_seq_copy['Ta'].iloc[t-12*4:t+2].mean()  # Average of next 12 hours\n",
    "    Ta_avg_24h = x_seq_copy['Ta'].iloc[t:t+2+24*4].mean()  # Average of next 12 to 24 hours\n",
    "    SIso_avg_4h = x_seq_copy['SIso'].iloc[t-4*2:t+2].mean()  # Average of next 4 hours\n",
    "    SIso_avg_8h = x_seq_copy['SIso'].iloc[t+2:t+2+8*4].mean()  # Average of next 4 to 8 hours\n",
    "    Ta_avg_next32h = x_seq_copy['Ta'].iloc[t+2:t+2+32*4].mean()  # Average of next 4 hours\n",
    "    Th_avg_last2 = x_seq_copy['Th'].iloc[t-2:t].mean()  # Average of next 4 to 8 hours\n",
    "    \n",
    "\n",
    "    delta_Tset=0\n",
    "    if x_seq_copy['Ti'].iloc[t+2] > max_t_eval:\n",
    "        delta_Tset = max_t_eval - x_seq_copy['Ti'].iloc[t+2]\n",
    "    elif x_seq_copy['Ti'].iloc[t+2] < min_t_eval:\n",
    "        delta_Tset = x_seq_copy['Ti'].iloc[t+2] - min_t_eval\n",
    "\n",
    "    # Extrahieren der Stunden und Minuten\n",
    "    timestamp = x_seq_copy.index[t + 2]\n",
    "    hours = timestamp.hour\n",
    "    minutes = timestamp.minute\n",
    "    hour_with_minutes = hours + minutes / 60.0\n",
    "\n",
    "    input_ = 0\n",
    "    norm_input = 0\n",
    "\n",
    "    if delta_Ti_2h_inp == 1 and post_Ta_avg_32h_inp == 1 and Tw_inp == 1 and Te_inp == 1 and price_flag == 0:\n",
    "        \n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1], next_result.var['Te'][-1], \n",
    "            deltaTi_2h, Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Ta_avg_next32h, Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw', 'Te', 'deltaTi', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Ta_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 1 and Te_inp == 1:\n",
    "\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1], next_result.var['Te'][-1], \n",
    "            Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw', 'Te',  'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 0 and Te_inp == 1:\n",
    "\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Te'][-1], \n",
    "            Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Te', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 1 and Te_inp == 0:\n",
    "\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1],\n",
    "            Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw',  'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    elif delta_Ti_2h_inp == 0 and post_Ta_avg_32h_inp == 0 and Tw_inp == 0 and Te_inp == 0:\n",
    "\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], \n",
    "            Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "\n",
    "    elif price_flag == 1 and no_future_inp == 0 and dif_price_inp == 0:\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1], next_result.var['Te'][-1], x_seq_copy['price'].iloc[t+2], x_seq_copy['price_min_in_x'].iloc[t+2], x_seq_copy['price_min_in_x'].iloc[t+2],         \n",
    "            deltaTi_2h, Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Ta_avg_next32h, Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw', 'Te', 'price', 'price', 'price', 'deltaTi', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Ta_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    elif price_flag == 1 and dif_price_inp == 1:\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1], next_result.var['Te'][-1], x_seq_copy['price'].iloc[t+2], x_seq_copy['price_delta'].iloc[t+2],         \n",
    "            deltaTi_2h, Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Ta_avg_next32h, Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' or  param == 'delta_price' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw', 'Te', 'price', 'deltaTi', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Ta_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    \n",
    "    elif price_flag == 1 and no_future_inp == 1:\n",
    "        input_ = np.array([\n",
    "            next_result.var['Ti'][-1], next_result.var['Tw'][-1], next_result.var['Te'][-1], x_seq_copy['price'].iloc[t+2],      \n",
    "            deltaTi_2h, Ta_avg_last12h, Ta_avg_24h, SIso_avg_4h, SIso_avg_8h, \n",
    "            Ta_avg_next32h, Th_avg_last2, hour_with_minutes, delta_Tset\n",
    "        ]) \n",
    "                \n",
    "        # Normalisiere Eingabedaten\n",
    "        norm_input = np.array([val if param == 'deltaTi' or  param == 'delta_Tset' else normalize_data(val, min_max_values[param]['min'], min_max_values[param]['max']) for val, param in zip(input_, \n",
    "            ['Ti', 'Tw', 'Te', 'price', 'deltaTi', 'Ta_avg', 'Ta_avg', 'SIso_avg', 'SIso_avg', 'Ta_avg', 'Th', 'day_hours', 'delta_Tset'])])\n",
    "\n",
    "    \n",
    "    return norm_input, hour_with_minutes, timestamp\n",
    "\n",
    "\n",
    "\n",
    "def feature_importance_test(sens_or_null,start_idx_w_pre, pop_idx,input_num,pre_sequence_length,post_sequence_length,x_seq,t,input_layer,\n",
    "                                        weights_biases,net_params):\n",
    "    feat_result = []\n",
    "    \n",
    "    for i_input in range(input_num+1):\n",
    "        \n",
    "        x_seq_copy_fit = x_seq\n",
    "        \n",
    "        reward_X_days_fit = []\n",
    "        energy_X_days_fit = []\n",
    "        distance_X_days_fit = []\n",
    "\n",
    "        bkt_state_last_fit = 1\n",
    "        \n",
    "        for t in range(pre_sequence_length, steps_X_days - post_sequence_length, 2):\n",
    "\n",
    "            if t == pre_sequence_length:  \n",
    "                bkt_state_last_fit = 1\n",
    "\n",
    "                ic_params_map_new = {\n",
    "                    'Ti0': lambda X, y, model_result: result_TRAIN.var['Ti'][start_idx_w_pre],\n",
    "                    'Tw0': lambda X, y, model_result: result_TRAIN.var['Tw'][start_idx_w_pre],\n",
    "                    'Te0': lambda X, y, model_result: result_TRAIN.var['Te'][start_idx_w_pre],\n",
    "                }\n",
    "            else:\n",
    "                ic_params_map_new = {\n",
    "                    'Ti0': lambda X, y, model_result: next_result_fit.var['Ti'][-1],\n",
    "                    'Tw0': lambda X, y, model_result: next_result_fit.var['Tw'][-1],\n",
    "                    'Te0': lambda X, y, model_result: next_result_fit.var['Te'][-1],\n",
    "                }\n",
    "\n",
    "            next_predict = predict_model(model_η_train, x_seq_copy_fit.iloc[t:t+2], y_seq.iloc[t:t+2], ic_params_map_new, error_metric, model_η_train.result)\n",
    "            next_result_fit = next_predict['model_result'].iloc[0]\n",
    "\n",
    "            norm_input_fit, hour_with_minutes_fit, timestamp_fit = create_new_input(x_seq_copy_fit,t,next_result_fit)\n",
    "            \n",
    "            if sens_or_null == True:\n",
    "                # Permutate:\n",
    "                zufalls_permutation = random.random()\n",
    "                if i_input != input_num:\n",
    "                    norm_input_fit[i_input] = zufalls_permutation\n",
    "            else: \n",
    "                # Nullify\n",
    "                if i_input != input_num:\n",
    "                    norm_input_fit[i_input] = 0\n",
    "            \n",
    "            input_layer = norm_input_fit.reshape(1, input_num)\n",
    "            normed_nn_out = forward_path_nnr(input_layer, weights_biases, net_params)\n",
    "\n",
    "            bkt_vorlauftemp = 0\n",
    "            # Denormalisiere NN-Ausgabedaten\n",
    "            if num_outputs == 1:\n",
    "                bkt_vorlauftemp = denormalize_data(normed_nn_out[0], min_max_values['Th']['min'], min_max_values['Th']['max'])\n",
    "            else:\n",
    "                bkt_vorlauftemp = denormalize_data(normed_nn_out[0][0], min_max_values['Th']['min'], min_max_values['Th']['max'])\n",
    "\n",
    "            bkt_vorlauftemp_15, bkt_vorlauftemp_30 = calculate_bkt_vorlauftemo(normed_nn_out,x_seq_copy_fit,t,next_result_fit)\n",
    "\n",
    "            bkt_state_last = bkt_an \n",
    "            \n",
    "            timestamp_t_plus_15 = x_seq_copy_fit.index[t] + timedelta(minutes=15)\n",
    "            timestamp_t_plus_30 = x_seq_copy_fit.index[t] + timedelta(minutes=30)\n",
    "            x_seq_copy_fit.at[timestamp_t_plus_15, 'Th'] = bkt_vorlauftemp_15\n",
    "            x_seq_copy_fit.at[timestamp_t_plus_30, 'Th'] = bkt_vorlauftemp_30\n",
    "\n",
    "            price_now = x_seq_copy_fit.at[timestamp_t_plus_15, 'price']\n",
    "            \n",
    "            new_Ti = next_result_fit.var['Ti'][-1]\n",
    "\n",
    "            # Evaluate Step\n",
    "            if t > (pre_sequence_length+ignor_first_half_day):\n",
    "                energy_taken = evaluate_energy(bkt_an,bkt_vorlauftemp_30,next_result.var['Tw'][-1],bkt_state_last, price_now) \n",
    "                e2_distance = evaluate_vorlauf_range(bkt_vorlauftemp) \n",
    "                e2_to_goal_temp = evaluate_temp(new_Ti,hour_with_minutes_fit)\n",
    "                reward_step = e2_to_goal_temp + e2_distance + energy_taken\n",
    "\n",
    "                energy_X_days_fit.append(energy_taken)\n",
    "                distance_X_days_fit.append(e2_distance)\n",
    "                abweichung_temp_X_days.append(e2_to_goal_temp)\n",
    "                reward_X_days_fit.append(reward_step)\n",
    "\n",
    "        # Evaluate example\n",
    "        reward_avg_permut_score_fit = np.mean(reward_X_days_fit)\n",
    "        feat_result.append(reward_avg_permut_score_fit)\n",
    "    \n",
    "    return feat_result\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(df, seq, seq_len, model, error_metric, same_start_time, start_time, predict_function, return_output=True, show_plot=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    df: DataFrame with the test dataset\n",
    "    seq: Number of sequences to test\n",
    "    seq_len: Length of each sequence in days\n",
    "    params: Model parameters (a, b, c, d)\n",
    "    same_start_time: If True, all sequences will start at the same time of day\n",
    "    start_time: Desired start time in hours for the sequences\n",
    "    \"\"\"\n",
    "    \n",
    "     # Calculate the sequence length in rows\n",
    "    sequence_length = int(seq_len * 24 * 4)  # seq_len days, 24 hours a day, 4 quarters of an hour per hour\n",
    "    \n",
    "    # Calculate the number of rows for the 6 hours pre-sequence\n",
    "    pre_sequence_length = 6 * 4\n",
    "    \n",
    "    if same_start_time:\n",
    "        # Calculate the maximum number of distinct sequences with the same start time\n",
    "        num_distinct_days = len(df) // (24 * 4)\n",
    "        max_sequences = min(num_distinct_days, seq)\n",
    "        \n",
    "        if seq > max_sequences:\n",
    "            warnings.warn(f\"Requested number of sequences {seq} exceeds the maximum \"\n",
    "                          f\"distinct sequences possible {max_sequences}. Using {max_sequences} sequences.\")\n",
    "            seq = max_sequences\n",
    "        \n",
    "        # Calculate the start indices for the sequences considering the start_time\n",
    "        day_interval = num_distinct_days // seq\n",
    "        start_indices = [(i * day_interval * 24 * 4) + (start_time * 4) - pre_sequence_length for i in range(seq)]\n",
    "    else:\n",
    "        # Calculate the start indices for the sequences considering the start_time\n",
    "        interval = (len(df) - sequence_length) // seq\n",
    "        start_indices = [(i * interval) + (start_time * 4) - pre_sequence_length for i in range(seq)]\n",
    "    \n",
    "    # Ensure the start indices are within the bounds of the DataFrame\n",
    "    start_indices = [max(index, 0) for index in start_indices]\n",
    "    \n",
    "    # Initialize a list to store the errors for each time step\n",
    "    errors = []\n",
    "    \n",
    "    for start_index in start_indices:\n",
    "        # Extract the sequence with pre-sequence from the DataFrame\n",
    "        sequence_df = df.iloc[start_index:start_index + sequence_length + pre_sequence_length].copy()\n",
    "        \n",
    "        # Calculate the weighted average for Te0 using the pre-sequence\n",
    "        weights = np.arange(1, pre_sequence_length + 1)\n",
    "        Agw = np.average(sequence_df.iloc[:pre_sequence_length]['Ta'], weights=weights)\n",
    "        Igw = np.average(sequence_df.iloc[:pre_sequence_length]['Ti'], weights=weights)\n",
    "        Te0 = Agw * 0.2 + Igw * 0.8\n",
    "        \n",
    "        # Update the initial condition for Te0 in the sequence\n",
    "        sequence_df.iloc[pre_sequence_length]['Te0'] = Te0\n",
    "        \n",
    "        # Extract the actual sequence for prediction after calculating Te0\n",
    "        sequence_df = sequence_df.iloc[pre_sequence_length:]\n",
    "        \n",
    "        # Call the abstract predict function to get the predictions\n",
    "        predictions = predict_function(sequence_df, model, error_metric, Te0)\n",
    "        \n",
    "        # Calculate the error for each time step and append it to the list\n",
    "        errors.append(np.abs(predictions - sequence_df['Ti'].to_numpy()))\n",
    "    \n",
    "    # Calculate the average error at each time step\n",
    "    average_errors = np.mean(errors, axis=0)\n",
    "    \n",
    "    # Calculate the standard deviation of the error at each time step\n",
    "    std_errors = np.std(errors, axis=0)\n",
    "    \n",
    "    if show_plot == True:\n",
    "        # Plot the average errors with standard deviation\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(0, sequence_length) / 4, average_errors, label='Average Error')\n",
    "        plt.fill_between(np.arange(0, sequence_length) / 4, average_errors - std_errors, average_errors + std_errors, alpha=0.2, label='Standard Deviation')\n",
    "        plt.xlabel('Time (hours)')\n",
    "        plt.ylabel('Error (°C)')\n",
    "        plt.title('Average Error of Predictions at Each Time Step with Standard Deviation')\n",
    "        plt.xticks(np.arange(0, sequence_length / 4 + 1, 6))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Find the first index where the average error exceeds 1°C and convert it to hours\n",
    "    first_exceed_index = np.argmax(average_errors > 0.4)\n",
    "    first_exceed_time = first_exceed_index / 4\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"The first prediction time where the average error exceeds 0.4°C is at {first_exceed_time} hours.\")\n",
    "    \n",
    "    # Create a DataFrame with the average_errors and std_errors\n",
    "    results_df = pd.DataFrame({\n",
    "        'average_errors': average_errors,\n",
    "        'std_errors': std_errors\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    results_df.to_csv('model_RC_results.csv', index=False)\n",
    "    \n",
    "    if return_output == True: \n",
    "        return errors\n",
    "    \n",
    "\n",
    "def calculate_weighted_means(pre_sequence_df):\n",
    "    # Berechne die gewichteten Mittelwerte von Agw und Igw\n",
    "    # Hier verwenden wir das DataFrame pre_sequence_df, das die Daten der 6 Stunden vor der Sequenz enthält\n",
    "    \n",
    "    # Gewichtungsfaktoren für die zeitliche Gewichtung, wobei die neueren Werte stärker gewichtet werden\n",
    "    weights = np.linspace(1, 2, len(pre_sequence_df))\n",
    "    \n",
    "    # Gewichteter Mittelwert von Agw\n",
    "    Agw = np.average(pre_sequence_df['Ta'], weights=weights)\n",
    "    \n",
    "    # Gewichteter Mittelwert von Igw (hier nehme ich an, dass 'Ti' die Innentemperatur ist)\n",
    "    Igw = np.average(pre_sequence_df['Ti'], weights=weights)\n",
    "    \n",
    "    return Agw, Igw\n",
    "\n",
    "\n",
    "def calculate_start_indices(df, seq, seq_len, same_start_time, start_time):\n",
    "    sequence_length = int(seq_len * 24 * 4)\n",
    "    \n",
    "    if same_start_time:\n",
    "        num_distinct_days = len(df) // (24 * 4)\n",
    "        max_sequences = min(num_distinct_days, seq)\n",
    "        if seq > max_sequences:\n",
    "            seq = max_sequences\n",
    "        day_interval = num_distinct_days // seq\n",
    "        start_indices = [(i * day_interval * 24 * 4) + (start_time * 4) for i in range(seq)]\n",
    "    else:\n",
    "        interval = (len(df) - sequence_length) // seq\n",
    "        start_indices = [(i * interval) + (start_time * 4) for i in range(seq)]\n",
    "    \n",
    "    return start_indices\n",
    "\n",
    "\n",
    "def extract_random_sequence_with_seed(df, series, length, seed):\n",
    "   \n",
    "    # Stelle sicher, dass die Länge nicht größer als die kleinste Länge von df und series ist\n",
    "    min_length = min(len(df), len(series))\n",
    "    if length > min_length:\n",
    "        raise ValueError(f\"Die angeforderte Länge {length} ist größer als die minimale Länge {min_length} von DataFrame oder Series.\")\n",
    "\n",
    "    # Initialisiere den Zufallszahlengenerator\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Finde gemeinsame Zeitstempel\n",
    "    common_timestamps = df.index.intersection(series.index)\n",
    "\n",
    "    # Wähle einen zufälligen Startpunkt\n",
    "    start_idx = np.random.choice(len(common_timestamps) - length + 1)\n",
    "    end_idx = start_idx + length\n",
    "\n",
    "    # Extrahiere die Sequenzen\n",
    "    selected_timestamps = common_timestamps[start_idx:end_idx]\n",
    "    df_seq = df.loc[selected_timestamps]\n",
    "    series_seq = series.loc[selected_timestamps]\n",
    "\n",
    "    return df_seq, series_seq, start_idx, end_idx\n",
    "\n",
    "def cut_data(df, series, anteil_train):\n",
    "    # Stelle sicher, dass der Anteil für die Trainingsdaten sinnvoll ist\n",
    "    if not (0 < anteil_train < 100):\n",
    "        raise ValueError(f\"Der angegebene Anteil für Trainingsdaten {anteil_train} ist nicht im Bereich von 0 bis 100.\")\n",
    "\n",
    "    # Finde gemeinsame Zeitstempel\n",
    "    common_timestamps = df.index.intersection(series.index)\n",
    "\n",
    "    # Berechne die Längen für Trainings- und Testdaten\n",
    "    total_length = len(common_timestamps)\n",
    "    length_train = int(total_length * anteil_train / 100)\n",
    "    length_test = total_length - length_train\n",
    "\n",
    "    # Extrahiere die Sequenzen für Trainingsdaten\n",
    "    start_idx_train = 0\n",
    "    end_idx_train = start_idx_train + length_train\n",
    "    selected_timestamps_train = common_timestamps[start_idx_train:end_idx_train]\n",
    "    df_seq_train = df.loc[selected_timestamps_train]\n",
    "    series_seq_train = series.loc[selected_timestamps_train]\n",
    "\n",
    "    # Extrahiere die Sequenzen für Testdaten\n",
    "    start_idx_test = end_idx_train\n",
    "    end_idx_test = start_idx_test + length_test\n",
    "    selected_timestamps_test = common_timestamps[start_idx_test:end_idx_test]\n",
    "    df_seq_test = df.loc[selected_timestamps_test]\n",
    "    series_seq_test = series.loc[selected_timestamps_test]\n",
    "\n",
    "    return df_seq_train, series_seq_train, start_idx_train, end_idx_train, df_seq_test, series_seq_test, start_idx_test, end_idx_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################### Load and Preprocess Data\n",
    "\n",
    "# Load ZSW-Data dataset\n",
    "data_path = './data/ZSW_Data_Full_2023-08-09_2023-09-26.csv'\n",
    "data_path_price = './data/day-ahead_price_quarter_hourly.csv'\n",
    "input_df = pd.read_csv(data_path, index_col=0, parse_dates=True)\n",
    "\n",
    "# Rename columns according to the mapping provided\n",
    "column_mapping = {\n",
    "    'ZSW_avg_Avg': 'Ti', #oder \"Schacht A_Avg\" oder \"Schacht B_Avg\"\n",
    "    'Tamb': 'Ta',\n",
    "    'BKT Temperatur': 'Th',\n",
    "    'SI_45': 'SIno',\n",
    "    'SI_135': 'SIso',\n",
    "    'Klimatisierung An/Aus':'BKT_an'\n",
    "}\n",
    "\n",
    "input_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add additional columns 'Ti0', 'Te0', and 'Tw0'\n",
    "input_df['Ti0'] = input_df['Ti']\n",
    "input_df['Tw0'] = input_df['Ti']  \n",
    "input_df['Te0'] = input_df['Ti'] \n",
    "\n",
    "# Remove other columns\n",
    "columns_to_keep = ['Th', 'Ta', 'SIno', 'SIso', 'Ti0', 'Te0', 'Tw0', 'Ti','BKT_an']\n",
    "input_df = input_df[columns_to_keep]\n",
    "\n",
    "# Check which columns have missing values\n",
    "missing_columns = input_df.columns[input_df.isna().any()].tolist()\n",
    "\n",
    "# Separate data into daily segments\n",
    "daily_segments = [group for name, group in input_df.groupby(input_df.index.date)]\n",
    "\n",
    "# Perform spline interpolation within each segment\n",
    "interpolated_segments = []\n",
    "for segment in daily_segments:\n",
    "    if len(segment) >= 4:  # Ensure segment has enough data points for cubic spline interpolation\n",
    "        interpolated_segment = segment.interpolate(method='spline', order=3)\n",
    "    else:\n",
    "        interpolated_segment = segment.interpolate(method='linear')  # Use linear interpolation for small segments\n",
    "    interpolated_segments.append(interpolated_segment)\n",
    "\n",
    "# Combine the segments back together\n",
    "interpolated_df = pd.concat(interpolated_segments)\n",
    "\n",
    "# Fill any remaining missing values at the edges\n",
    "interpolated_df = interpolated_df.ffill().bfill()\n",
    "\n",
    "# Update the input_df with interpolated values\n",
    "input_df[missing_columns] = interpolated_df[missing_columns]\n",
    "\n",
    "\n",
    "\n",
    "# Update the input_df with interpolated values\n",
    "input_df[missing_columns] = interpolated_df[missing_columns]\n",
    "\n",
    "# Laden der day-ahead_price_quarter_hourly.csv-Datei\n",
    "day_ahead_data = pd.read_csv(data_path_price, index_col='datetime', parse_dates=True)\n",
    "\n",
    "# Konvertieren von Zeitstempeln in day_ahead_data in tz-naive (wenn sie tz-aware sind)\n",
    "if day_ahead_data.index.tz is not None:\n",
    "    day_ahead_data.index = day_ahead_data.index.tz_localize(None)\n",
    "\n",
    "# Konvertieren von Zeitstempeln in input_df in tz-naive (wenn sie tz-aware sind)\n",
    "if input_df.index.tz is not None:\n",
    "    input_df.index = input_df.index.tz_localize(None)\n",
    "\n",
    "# Zusammenführen der Daten basierend auf dem Index (Datum und Uhrzeit)\n",
    "# Hierbei wird die Spalte aus day_ahead_data direkt als 'day_ahead_price' in input_df eingefügt\n",
    "input_df = input_df.merge(day_ahead_data['day_ahead_auction'].rename('price'), left_index=True, right_index=True, how='left')\n",
    "\n",
    "def add_min_max_future_steps(df, look_future_steps):\n",
    "    # Berechnen des minimalen Werts in den nächsten 'look_future_steps' Schritten\n",
    "    df[f'price_min_in_x'] = df['price'].rolling(window=look_future_steps, min_periods=1).min().shift(-look_future_steps + 1)\n",
    "\n",
    "    # Berechnen des maximalen Werts in den nächsten 'look_future_steps' Schritten\n",
    "    df[f'price_max_in_x'] = df['price'].rolling(window=look_future_steps, min_periods=1).max().shift(-look_future_steps + 1)\n",
    "\n",
    "    df['price_min_in_x'] = df['price_min_in_x'].ffill()\n",
    "    df['price_max_in_x'] = df['price_max_in_x'].ffill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "# Ersetzen Sie dies durch die gewünschte Anzahl an zukünftigen Schritten\n",
    "input_df = add_min_max_future_steps(input_df, look_future_steps)\n",
    "\n",
    "# Umwandeln der 'price'-Spalte in einen Float-Typ\n",
    "def convert_columns_to_numeric(df, column_names):\n",
    "    for column in column_names:\n",
    "        if column in df.columns:\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "# Konvertieren der relevanten Spalten in numerische Werte\n",
    "columns_to_convert = ['price', 'price_min_in_x', 'price_max_in_x']\n",
    "convert_columns_to_numeric(input_df, columns_to_convert)\n",
    "\n",
    "# Extract input features and target variable\n",
    "input_X = input_df[['Th', 'Ta', 'SIno', 'SIso', 'Ti0', 'Te0', 'Tw0', 'price', 'price_min_in_x', 'price_max_in_x']]\n",
    "input_y = input_df['Ti']\n",
    "\n",
    "# Print shapes of input features and target variable\n",
    "#print(f'Input X shape: {input_X.shape}, input y shape: {input_y.shape}')\n",
    "\n",
    "# The duration of a record\n",
    "rec_duration = 0.25  # hour\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################### Create Train/Test Data\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_X, input_y, test_size=5 / 33, shuffle=False)\n",
    "\n",
    "# Print shapes of training and test sets\n",
    "#print(f'Train: X shape: {X_train.shape}, y shape: {y_train.shape}')\n",
    "#print(f'Test: X shape: {X_test.shape}, y shape: {y_test.shape}')\n",
    "\n",
    "# Check for missing values (NaN) in the dataset\n",
    "nan_counts = input_df.isna().sum()\n",
    "nan_counts[nan_counts > 0]\n",
    "\n",
    "#print(\"\")\n",
    "#print(nan_counts)\n",
    "#print(\"\")\n",
    "#print(input_df)\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#plot_input_data(input_df, with_Kh = False, with_Ph = False)\n",
    "\n",
    "\n",
    "\n",
    "#################################### Erzeuge oder Lade RC-Modell und Modell configs and Splits\n",
    "\n",
    "train_params_TiTeTwSInoSIso = {\n",
    "    'Ti0': {'value': 35, 'vary': False},\n",
    "    'Te0': {'value': 10, 'vary': False},\n",
    "    'Tw0': {'value': 30, 'vary': False},    \n",
    "    'Ci': {'value': 12.41119259, 'min': 0.1, 'max': 400},\n",
    "    'Ce': {'value': 436.080547 ,'min': 0.2, 'max': 2000},\n",
    "    'Cw': {'value': 170.128391 ,'min': 0.2, 'max': 2000},\n",
    "    'Rie': {'value': 2.60363635, 'min': 0.001, 'max': 500},\n",
    "    'Rea': {'value': 4.73124347, 'min': 0.001, 'max': 500},\n",
    "    'Riw': {'value': 3.61742182, 'min': 0.001, 'max': 500},\n",
    "    'Rhw': {'value': 0.40619, 'min': 0.001, 'max': 500},\n",
    "    'Aino': {'value': 0.00279975, 'min': 0.0000001, 'max': 30},\n",
    "    'Aiso': {'value': 0.00494192, 'min': 0.0000001, 'max': 30},\n",
    "}\n",
    "\n",
    "ic_params_map = {\n",
    "    'Ti0': lambda X_test, y_test, train_result: y_train.iloc[0],\n",
    "    'Tw0': lambda X_test, y_test, train_result: y_train.iloc[0],\n",
    "    'Te0': lambda X_test, y_test, train_result: train_result.var['Te'][-1],\n",
    "}\n",
    "\n",
    "models = [\n",
    "          #TiTeTh(train_params_TiTeTh, rec_duration),\n",
    "          TiTeTwSInoSIso(train_params_TiTeTwSInoSIso, rec_duration),\n",
    "          #TiTeThSIη(train_params_TiTeThSIη, rec_duration),\n",
    "        ]\n",
    "\n",
    "prefit_splits = custom_splits(X_train, num_sets=1, train_len=60, val_len=20, variance=70)\n",
    "#splits_list = list(prefit_splits)\n",
    "\n",
    "#prefit_splits = KFold(n_splits=int(len(X_train) / 96), shuffle=False).split(X_train)\n",
    "\n",
    "error_metric = rmse\n",
    "\n",
    "prefit_filter = lambda error: abs(error) < 10\n",
    "\n",
    "method = 'nelder'\n",
    "#method = 'leastsq'\n",
    "\n",
    "#################################### Train RC-Modell\n",
    "\n",
    "df = darkgreyfit(models, X_train, y_train, X_test, y_test, ic_params_map, error_metric,\n",
    "                 prefit_splits=prefit_splits, prefit_filter=prefit_filter, reduce_train_results=True, \n",
    "                 method=method, n_jobs=-1, verbose=10)\n",
    "\n",
    "\n",
    "#################################### Select Modell\n",
    "\n",
    "pos = 0\n",
    "\n",
    "# Example usage: selecting the model at position 1\n",
    "model_η_train = select_model_by_position(df, pos)\n",
    "\n",
    "train_results_η_train = df.loc[pos, ('train', 'model_result')]\n",
    "test_results_η_train = df.loc[pos, ('test', 'model_result')]\n",
    "\n",
    "# model_η_train.result.params\n",
    "\n",
    "#################################### Plot Modell Simulation Results\n",
    "\n",
    "#plot_with_Th(y_train, train_results_η_train, 'TiTeThSIη (Train)', X_with_out=None, with_Tw=True)\n",
    "#plot_with_Th(y_train, train_results_η_train, 'TiTeThSIη (Train)', X_with_out=X_train, with_Th=True, with_Ta=True, with_SIno=True, with_SIso=True, with_Tw=True)\n",
    "#plot_with_Th(y_test, test_results_η_train, 'TiTeThSIη (Train)', X_with_out=None, with_Tw=True)\n",
    "#plot_with_Th(y_test, test_results_η_train, 'TiTeThSIη (Train)', X_with_out=X_test,with_Th=True, with_Ta=True, with_SIno=True, with_SIso=True, with_Tw=True)\n",
    "\n",
    "#################################### Override Modell to load old values\n",
    "\n",
    "model_η_train.result.params['Ci'].value = 27.8315427\n",
    "model_η_train.result.params['Ce'].value = 585.492669\n",
    "model_η_train.result.params['Cw'].value = 132.201565\n",
    "model_η_train.result.params['Rie'].value = 0.93187567\n",
    "model_η_train.result.params['Rea'].value = 0.46359172\n",
    "model_η_train.result.params['Riw'].value = 0.37819626\n",
    "model_η_train.result.params['Rhw'].value = 0.67818813\n",
    "model_η_train.result.params['Aino'].value = 0.00899538\n",
    "model_η_train.result.params['Aiso'].value = 0.01125552\n",
    "\n",
    "\n",
    "#################################### Make Prediction for Data\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_TRAIN, X_TEST, y_TRAIN, y_TEST = train_test_split(input_X, input_y, test_size = 5 / 33, shuffle=False)\n",
    "\n",
    "ic_params_map_start = {\n",
    "    'Ti0': lambda X, y, model_result: y_TRAIN.iloc[0],\n",
    "    'Tw0': lambda X, y, model_result: y_TRAIN.iloc[0],\n",
    "    'Te0': lambda X, y, model_result: y_TRAIN.iloc[0],\n",
    "}\n",
    "\n",
    "prediction_TRAIN = predict_model(model_η_train, X_TRAIN, y_TRAIN, ic_params_map_start, error_metric, model_η_train.result)\n",
    "\n",
    "result_TRAIN = prediction_TRAIN['model_result'].iloc[0]  \n",
    "\n",
    "#plot_with_Th(y_TRAIN, result_TRAIN, 'TiTeThSIη (Train)', X_with_out=None, with_Tw=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "######################################################## NN-Controller ###########################################################\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Netzwerkstruktur festlegen\n",
    "structure = [input_num, num_hidden_neurons, num_outputs]  # Eingabe hat 2 Merkmale, eine versteckte Schicht mit 4 Neuronen und eine Ausgabeschicht mit 1 Neuron\n",
    "\n",
    "# Anzahl der Parameter berechnen und PGPE-Optimierer initialisieren\n",
    "num_paras = get_num_paras_from_structure(structure)\n",
    "para_dict = OP()\n",
    "\n",
    "for i in range(num_paras):\n",
    "    para_dict.add_parameter(\n",
    "        key=f\"param_{i}\",\n",
    "        min=-10.0, max=10.0, type=\"f\", scale=\"linear\", init=0.0\n",
    "    )\n",
    "\n",
    "np.random.seed(169)\n",
    "start_seed = np.random.randint(0, 10000)\n",
    "initial_params = np.array([para_dict.get_init_parameters_array()])\n",
    "seed_int = int(np.sum(initial_params))\n",
    "para_dict = para_dict.parameter_dict\n",
    "optimizer = RankSuperSymPGPE(para_dict, plot_paras=False, seed=initial_params)\n",
    "\n",
    "optimizer.mAlpha = mue_alpha\n",
    "optimizer.sAlpha = sigma_alpha\n",
    "\n",
    "# Define the PT1 function\n",
    "def use_pt1_function(t, beharrungswert, start_temp, T = 1506.6743): \n",
    "    return beharrungswert - (beharrungswert - start_temp) * np.exp(-t / T)\n",
    "\n",
    "#Grenzen des Heiz- und Kühlfalls:\n",
    "kühl_temp_min = 17.5\n",
    "kühl_temp_max = 20.5\n",
    "heiz_temp_min = 22\n",
    "heiz_temp_max = 28\n",
    "\n",
    "#Arbeitszeiten definieren\n",
    "start_working_hour = 6\n",
    "end_working_hour = 20\n",
    "    \n",
    "# Define a manual color scheme for the plots\n",
    "color_scheme = {\n",
    "    'Ti': 'blue', 'Th': 'orange', 'Tw': 'green',\n",
    "    'Ta': 'red', 'Te': 'purple', 'SIno': 'brown', 'SIso': 'pink', \n",
    "    'price': 'olive','price_min_in_x': 'navy','price_max_in_x': 'maroon'\n",
    "}\n",
    "\n",
    "X_TRAIN.rename(columns={'Ti0': 'Ti'}, inplace=True)\n",
    "\n",
    "save_path = new_folder_path ##'./XXXX/ExampleRun/'\n",
    "num_samples = num_iterations\n",
    "\n",
    "#Init Variables\n",
    "steps_X_days = 0\n",
    "ignor_first_half_day = 0\n",
    "take_test_data = False\n",
    "avg_reward_seq_model = 0\n",
    "last_Iteration = False\n",
    "best_test = -999\n",
    "best_test_score = -999.9\n",
    "test_scores = []\n",
    "test_scores_index = []\n",
    "result_TRAIN_copy = result_TRAIN\n",
    "\n",
    "# Initialize dictionaries to store the reward history\n",
    "reward_history = {pop: [] for pop in range(optimizer.pop_size)}\n",
    "model_reward_history = []\n",
    "final_weights_biases = []\n",
    "\n",
    "\n",
    "print(\"#####################################\")\n",
    "print(\"###########  Starting RL  ###########\")\n",
    "\n",
    "\n",
    "for i in range(num_samples):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\" ########### I: {i} ############\")\n",
    "    \n",
    "    if i % take_test_data_every == 0 and i != 0:\n",
    "        take_test_data = True\n",
    "    else: \n",
    "        take_test_data = False\n",
    "    \n",
    "    if i == (num_samples-1):\n",
    "        last_Iteration = True\n",
    "    \n",
    "    if i % make_plot_each == 0 or take_test_data == True:\n",
    "        modell_results_dict = {pop: [] for pop in range(optimizer.pop_size)}\n",
    "    \n",
    "    # Adjust sequence extraction to include pre- and post-sequences\n",
    "    x_seq, y_seq, start_idx, end_idx, x_seq_test, y_seq_test, start_idx_test, end_idx_test = cut_data(df=X_TRAIN, series=y_TRAIN, anteil_train=85)\n",
    "    \n",
    "    if take_test_data == True:\n",
    "        start_idx = start_idx_test\n",
    "        end_idx = end_idx_test\n",
    "        x_seq = x_seq_test\n",
    "        y_seq = y_seq_test\n",
    "    \n",
    "    steps_X_days = end_idx - start_idx\n",
    "    pre_sequence_length = 12*4  # 2 hours back, 4 timesteps per hour\n",
    "    post_sequence_length = 32*4+2  # 24 hours forward, 4 timesteps per hour\n",
    "    \n",
    "    # Adjust start_idx and end_idx to account for pre-sequence\n",
    "    start_idx_w_pre = start_idx + pre_sequence_length\n",
    "    end_idx_w_post = end_idx - post_sequence_length\n",
    "    \n",
    "    population = optimizer.ask()\n",
    "    rewards = []\n",
    "    reward_avgs = []\n",
    "    avgs_energy_X_days = []\n",
    "    avgs_distance_X_days = []\n",
    "    avgs_abweichung_temp_X_days = []\n",
    "    \n",
    "    bkt_an = 1\n",
    "    bkt_state_last = 1\n",
    "    \n",
    "    feat_results = []\n",
    "    \n",
    "    for pop_idx, para_vector in enumerate(population):\n",
    "        \n",
    "        x_seq_copy = x_seq\n",
    "        weights_biases, net_params = make_weights_biases_from_vector(para_vector, structure)\n",
    "        \n",
    "        reward_X_days = []\n",
    "        energy_X_days = []\n",
    "        distance_X_days = []\n",
    "        abweichung_temp_X_days = []\n",
    "        \n",
    "        # Feature Importance\n",
    "        feat_importance_results = []\n",
    "        if (last_Iteration == True or i % update_feat_every == 0) and i > no_feat_imp_bevore:\n",
    "            final_weights_biases.append(weights_biases)\n",
    "            \n",
    "            sens_feat_results = feature_importance_test(True,start_idx_w_pre, pop_idx,input_num,pre_sequence_length,post_sequence_length,x_seq_copy,t,bkt_state_last,input_layer,\n",
    "                                    weights_biases,net_params)\n",
    "            null_feat_results = feature_importance_test(False,start_idx_w_pre, pop_idx,input_num,pre_sequence_length,post_sequence_length,x_seq_copy,t,bkt_state_last,input_layer,\n",
    "                                    weights_biases,net_params)\n",
    "         \n",
    "            feat_results.append([sens_feat_results,null_feat_results])\n",
    "        \n",
    "        for t in range(pre_sequence_length, steps_X_days - post_sequence_length, 2):\n",
    "            \n",
    "            if t == pre_sequence_length:  \n",
    "                bkt_state_last = 1\n",
    "                \n",
    "                ic_params_map_new = {\n",
    "                    'Ti0': lambda X, y, model_result: result_TRAIN.var['Ti'][start_idx_w_pre],\n",
    "                    'Tw0': lambda X, y, model_result: result_TRAIN.var['Tw'][start_idx_w_pre],\n",
    "                    'Te0': lambda X, y, model_result: result_TRAIN.var['Te'][start_idx_w_pre],\n",
    "                }\n",
    "            else:\n",
    "                ic_params_map_new = {\n",
    "                    'Ti0': lambda X, y, model_result: next_result.var['Ti'][-1],\n",
    "                    'Tw0': lambda X, y, model_result: next_result.var['Tw'][-1],\n",
    "                    'Te0': lambda X, y, model_result: next_result.var['Te'][-1],\n",
    "                }\n",
    "\n",
    "            next_predict = predict_model(model_η_train, x_seq_copy.iloc[t:t+2], y_seq.iloc[t:t+2], ic_params_map_new, error_metric, model_η_train.result)\n",
    "            next_result = next_predict['model_result'].iloc[0]\n",
    "            \n",
    "            norm_input, hour_with_minutes, timestamp = create_new_input(x_seq_copy,t,next_result)\n",
    "            \n",
    "            input_layer = norm_input.reshape(1, input_num)\n",
    "            normed_nn_out = forward_path_nnr(input_layer, weights_biases, net_params)\n",
    "            \n",
    "            bkt_vorlauftemp = 0\n",
    "            \n",
    "            # Denormalisiere NN-Ausgabedaten\n",
    "            if num_outputs == 1:\n",
    "                bkt_vorlauftemp = denormalize_data(normed_nn_out[0], min_max_values['Th']['min'], min_max_values['Th']['max'])\n",
    "            else:\n",
    "                bkt_vorlauftemp = denormalize_data(normed_nn_out[0][0], min_max_values['Th']['min'], min_max_values['Th']['max'])\n",
    "                \n",
    "            bkt_vorlauftemp_15, bkt_vorlauftemp_30 = calculate_bkt_vorlauftemo(normed_nn_out,x_seq_copy,t,next_result)\n",
    "            \n",
    "            bkt_state_last = bkt_an \n",
    "            \n",
    "            timestamp_t_plus_15 = x_seq_copy.index[t] + timedelta(minutes=15)\n",
    "            timestamp_t_plus_30 = x_seq_copy.index[t] + timedelta(minutes=30)\n",
    "            x_seq_copy.at[timestamp_t_plus_15, 'Th'] = bkt_vorlauftemp_15\n",
    "            x_seq_copy.at[timestamp_t_plus_30, 'Th'] = bkt_vorlauftemp_30\n",
    "            \n",
    "            new_Ti = next_result.var['Ti'][-1]\n",
    "            \n",
    "            if i % make_plot_each == 0 or take_test_data == True:\n",
    "                \n",
    "                timestamp = x_seq_copy.index[t+2]\n",
    "                new_row = 0\n",
    "                if price_flag == 1:\n",
    "                    if no_future_inp == 1:\n",
    "                        new_row = { 'Timestamp': timestamp, 'Ti': next_result.var['Ti'][-1], 'Tw': next_result.var['Tw'][-1], 'Te': next_result.var['Te'][-1], 'Ta': x_seq_copy['Ta'].iloc[t+2], 'Th': x_seq_copy['Th'].iloc[t+2],'price':x_seq_copy['price'].iloc[t+2]}\n",
    "                    else:\n",
    "                        new_row = { 'Timestamp': timestamp, 'Ti': next_result.var['Ti'][-1], 'Tw': next_result.var['Tw'][-1], 'Te': next_result.var['Te'][-1], 'Ta': x_seq_copy['Ta'].iloc[t+2], 'Th': x_seq_copy['Th'].iloc[t+2],'price':x_seq_copy['price'].iloc[t+2],'price_min_in_x': x_seq_copy['price_min_in_x'].iloc[t+2], 'price_max_in_x':x_seq_copy['price_max_in_x'].iloc[t+2] }\n",
    "                else:\n",
    "                    new_row = { 'Timestamp': timestamp, 'Ti': next_result.var['Ti'][-1], 'Tw': next_result.var['Tw'][-1], 'Te': next_result.var['Te'][-1], 'Ta': x_seq_copy['Ta'].iloc[t+2], 'Th': x_seq_copy['Th'].iloc[t+2], 'SIno': x_seq_copy['SIno'].iloc[t+2], 'SIso': x_seq_copy['SIso'].iloc[t+2] }\n",
    "\n",
    "                modell_results_dict[pop_idx].append(new_row)\n",
    "            \n",
    "            # Evaluate Step\n",
    "            if t > (pre_sequence_length+ignor_first_half_day):\n",
    "                price_now = x_seq_copy.at[timestamp_t_plus_15, 'price']\n",
    "                energy_taken = evaluate_energy(bkt_an,bkt_vorlauftemp_30,next_result.var['Tw'][-1],bkt_state_last, price_now) \n",
    "                e2_distance = evaluate_vorlauf_range(bkt_vorlauftemp) \n",
    "                e2_to_goal_temp = evaluate_temp(new_Ti,hour_with_minutes)\n",
    "                reward_step = e2_to_goal_temp + e2_distance + energy_taken\n",
    "\n",
    "                energy_X_days.append(energy_taken)\n",
    "                distance_X_days.append(e2_distance)\n",
    "                abweichung_temp_X_days.append(e2_to_goal_temp)\n",
    "                reward_X_days.append(reward_step)\n",
    "\n",
    "        # Evaluate example\n",
    "        avg_energy_X_days = np.mean(energy_X_days)\n",
    "        avg_distance_X_days = np.mean(distance_X_days)\n",
    "        avg_abweichung_temp_X_days = np.mean(abweichung_temp_X_days)\n",
    "        avgs_energy_X_days.append(avg_energy_X_days)\n",
    "        avgs_distance_X_days.append(avg_distance_X_days)\n",
    "        avgs_abweichung_temp_X_days.append(avg_abweichung_temp_X_days)\n",
    "        \n",
    "        reward_avg = np.mean(reward_X_days)\n",
    "        rewards.append(reward_avg)\n",
    "        reward_avgs.append(reward_avg)\n",
    "    \n",
    "    # Update reward history after each iteration\n",
    "    for pop_idx in range(optimizer.pop_size):\n",
    "        reward_history[pop_idx].append(reward_avgs[pop_idx])\n",
    "    model_reward_history.append(avg_reward_seq_model)\n",
    "    \n",
    "    if (last_Iteration == True or i % update_feat_every == 0) and i > no_feat_imp_bevore:\n",
    "        feat_res_copy = feat_results\n",
    "        save_feat_csv(feat_res_copy)\n",
    "        if last_Iteration == True:\n",
    "            print(\"\")\n",
    "            print(\"##########  Done with RL  ###########\")\n",
    "            print(\"#####################################\")\n",
    "    \n",
    "    if i % make_plot_each == 0 or take_test_data == True:\n",
    "        \n",
    "        print(\"  \")\n",
    "        print(\" Erstelle 3x2-Subplot-Raster \")\n",
    "\n",
    "        # Create a 3x2 subplot grid\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(20, 15))\n",
    "        axs = axs.flatten()  # Flache Liste für einfachen Zugriff\n",
    "        \n",
    "        # Adjust spacing between plots\n",
    "        plt.subplots_adjust(hspace=0.35, wspace=0.17)\n",
    "\n",
    "        # Fill the first 2x2 grid\n",
    "        for pop_idx in range(optimizer.pop_size):\n",
    "            modell_results = pd.DataFrame(modell_results_dict[pop_idx])\n",
    "            if not modell_results.empty:\n",
    "                for column in modell_results.columns[1:]:\n",
    "                    color = color_scheme.get(column, 'black')  # Default to black if not found\n",
    "                    \"\"\"\n",
    "                    if column in ['price_min_in_x','price_max_in_x']:\n",
    "                        new_label = 'p_min' if column == 'price_min_in_x' else 'p_max'\n",
    "                        axs[pop_idx].plot(modell_results['Timestamp'], modell_results[column] / 10, label=new_label, color=color, alpha=0.4)\n",
    "                    elif column in ['price']:\n",
    "                        axs[pop_idx].plot(modell_results['Timestamp'], modell_results[column] / 10, label=column, color=color)\n",
    "                    elif column in ['SIno', 'SIso']:\n",
    "                        axs[pop_idx].plot(modell_results['Timestamp'], modell_results[column] / 1000, label=column, color=color)\n",
    "                    else:\n",
    "                        axs[pop_idx].plot(modell_results['Timestamp'], modell_results[column], label=column, color=color)\n",
    "                    \"\"\"\n",
    "                    if column in ['Ti','Th','Ta']:\n",
    "                        new_label = 'default' \n",
    "                        if column == 'Ti':\n",
    "                            new_label = 'Temp. Raum'\n",
    "                        elif column == 'Th':\n",
    "                            new_label = 'Temp. Heiz'\n",
    "                        elif column == 'Ta':\n",
    "                            new_label = 'Temp. Außen'\n",
    "\n",
    "                        axs[pop_idx].plot(modell_results['Timestamp'], modell_results[column], label=new_label, color=color)\n",
    "                        \n",
    "                axs[pop_idx].fill_between(modell_results['Timestamp'], max_t_eval, min_t_eval, color='green', alpha=0.53)\n",
    "                axs[pop_idx].legend(loc='upper right')\n",
    "                axs[pop_idx].set_title(f'Population {pop_idx + 1} - Avg Reward: {reward_avgs[pop_idx]:.2f}\\nTemp: {avgs_abweichung_temp_X_days[pop_idx]:.2f} - Energy: {avgs_energy_X_days[pop_idx]:.2f} - Range: {avgs_distance_X_days[pop_idx]:.2f}')\n",
    "                axs[pop_idx].grid(True)  # Add grid\n",
    "                axs[pop_idx].set_ylabel('Values')  # Add y-axis label\n",
    "                axs[pop_idx].xaxis.set_major_locator(MaxNLocator(3))  # Limit x-axis ticks\n",
    "        \n",
    "        \n",
    "        # Adjusted index range to match the range used in the loop for population data\n",
    "        adjusted_start_idx = start_idx_w_pre   # Start from the first data point  pre_sequence_length\n",
    "        adjusted_end_idx = end_idx_w_post  # End at the last data point\n",
    "\n",
    "        # Extract timestamps from y_TRAIN for the relevant indices\n",
    "        timestamps = y_TRAIN.index[adjusted_start_idx:adjusted_end_idx]\n",
    "        \n",
    "        # Create the DataFrame for seq_modell_df with the adjusted range\n",
    "        seq_modell_data = {\n",
    "            **{key: value[adjusted_start_idx:adjusted_end_idx] for key, value in result_TRAIN.X.items()},\n",
    "            **{f'var_{key}': value[adjusted_start_idx:adjusted_end_idx] for key, value in result_TRAIN.var.items()},\n",
    "            'BKT_an': input_df['BKT_an'][adjusted_start_idx:adjusted_end_idx]\n",
    "        }\n",
    "        seq_modell_df = pd.DataFrame(seq_modell_data)  \n",
    "\n",
    "        # Add the 'Timestamp' column to seq_modell_df\n",
    "        seq_modell_df['Timestamp'] = timestamps\n",
    "        \n",
    "        # Rename columns as specified\n",
    "        seq_modell_df.rename(columns={'var_Ti': 'Ti', 'var_Te': 'Te', 'var_Tw': 'Tw'}, inplace=True)\n",
    "        \n",
    "        # Anwenden der shift()-Funktion und Ersetzen des ersten NaN-Wertes durch 1\n",
    "        seq_modell_df['BKT_an_shifted'] = seq_modell_df['BKT_an'].shift(1).fillna(1)\n",
    "        \n",
    "        # Entfernen der ersten 12 Stunden aus seq_modell_df\n",
    "        seq_modell_df_shortened = seq_modell_df.iloc[ignor_first_half_day:].copy()\n",
    "        \n",
    "        # Berechnen des durchschnittlichen Rewards\n",
    "        energy_seq_model = [evaluate_energy(an, vorlauftemp, tw, last_state, price) \n",
    "                            for an, vorlauftemp, tw, last_state, price in zip(seq_modell_df_shortened['BKT_an'], \n",
    "                                                                       seq_modell_df_shortened['Th'], \n",
    "                                                                       seq_modell_df_shortened['Tw'], \n",
    "                                                                       seq_modell_df_shortened['BKT_an_shifted'],\n",
    "                                                                       seq_modell_df_shortened['price'])]\n",
    "        \n",
    "        # Berechnen von 'hour_with_minutes' aus der 'Timestamp'-Spalte\n",
    "        seq_modell_df_shortened['hour_with_minutes'] = [timestamp.hour + timestamp.minute / 60.0 for timestamp in seq_modell_df_shortened['Timestamp']]\n",
    "\n",
    "        # Anpassen des Aufrufs von evaluate_temp\n",
    "        temp_seq_model = [evaluate_temp(ti, hour_with_minutes) for ti, hour_with_minutes in zip(seq_modell_df_shortened['Ti'], seq_modell_df_shortened['hour_with_minutes'])]\n",
    "\n",
    "        avg_energy_seq_model = np.mean(energy_seq_model)\n",
    "        avg_temp_seq_model = np.mean(temp_seq_model)\n",
    "        rewards_seq_model = avg_temp_seq_model + avg_energy_seq_model\n",
    "        avg_reward_seq_model = rewards_seq_model\n",
    "\n",
    "        # Plot the new script's data in the third row\n",
    "        for column in ['Ti', 'Th', 'Tw', 'Ta', 'Te', 'SIno', 'SIso']:  # Ensure the order\n",
    "            if column in seq_modell_df.columns:\n",
    "                plot_data = seq_modell_df[column] / 1000 if column in ['SIno', 'SIso'] else seq_modell_df[column]\n",
    "                axs[4].plot(seq_modell_df['Timestamp'], plot_data, label=column, color=color_scheme[column])\n",
    "        \n",
    "        # Add a grid, y-axis label, and limit x-axis ticks for the bottom plot\n",
    "        axs[4].fill_between(modell_results['Timestamp'], max_t_eval, min_t_eval, color='green', alpha=0.2)\n",
    "        axs[4].grid(True)\n",
    "        axs[4].set_ylabel('Values')\n",
    "        axs[4].xaxis.set_major_locator(MaxNLocator(3))\n",
    "        axs[4].set_xlabel('Timestamp')\n",
    "        axs[4].set_title(f'Seq Model - Avg Reward: {rewards_seq_model:.2f}\\nTemp: {avg_temp_seq_model:.2f} - Energy: {avg_energy_seq_model:.2f}')\n",
    "        axs[4].legend(loc='upper right')\n",
    "\n",
    "        # Remove the unused sixth subplot\n",
    "        fig.delaxes(axs[5])\n",
    "        \n",
    "        # Calculate and display moving averages\n",
    "        moving_avg_rewards = {pop: np.mean(reward_history[pop][-5:]) if len(reward_history[pop]) >= 5 else np.mean(reward_history[pop]) for pop in range(optimizer.pop_size)}\n",
    "        moving_avg_model_reward = np.mean(model_reward_history[-5:]) if len(model_reward_history) >= 5 else np.mean(rewards_seq_model)\n",
    " \n",
    "        # Position for the moving average text\n",
    "        text_x, text_y_start = 1.2, 0.8\n",
    "        text_font_size = 20  # Adjust this value as needed\n",
    "        delta_y = 0.13\n",
    "\n",
    "        # Display moving averages for each population\n",
    "        for pop_idx in range(optimizer.pop_size):\n",
    "            axs[4].text(text_x, text_y_start - pop_idx * delta_y, \n",
    "                        f\"Moving Avg Reward Pop {pop_idx + 1}: {moving_avg_rewards[pop_idx]:.2f}\", \n",
    "                        transform=axs[4].transAxes, fontsize=text_font_size)\n",
    "\n",
    "        # Display moving average for the model\n",
    "        axs[4].text(text_x, text_y_start - len(reward_history) * delta_y, \n",
    "                    f\"Moving Avg Reward Model: {moving_avg_model_reward:.2f}\", \n",
    "                    transform=axs[4].transAxes, fontsize=text_font_size)\n",
    "        \n",
    "        plt.savefig(save_path + f'modell_results_combined_{i}.png')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        modell_results_dict = {pop: [] for pop in range(optimizer.pop_size)}\n",
    "        \n",
    "    if take_test_data == False:\n",
    "        \n",
    "        optimizer.tell(population, np.array(rewards), None, plot_path=save_path)\n",
    "    else:\n",
    "        # Check Test Run\n",
    "        test_score_now = max(rewards)\n",
    "        if i > 1000:\n",
    "            if best_test_score < test_score_now:\n",
    "                best_test_score = test_score_now\n",
    "                best_test = i\n",
    "                \n",
    "        # Hinzufügen der Werte zu test_scores und test_scores_index\n",
    "        test_scores.append(test_score_now)\n",
    "        test_scores_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d8338-4889-4740-925e-047d6eeefec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
